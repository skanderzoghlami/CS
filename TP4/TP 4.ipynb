{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TP 4  \n",
    "Ce TP regroupe le chapitre 5 (partie 2)  du cours calcul scientifique. <br>\n",
    "\n",
    "**Deadline 14 Mai 23h59**\n",
    "\n",
    "Ce TP est noté sur 15 points répartis comme suit: \n",
    "    - Exercice: 13 points\n",
    "    - Qualité du code, commentaires et interprétations: 2 points\n",
    "    \n",
    "    \n",
    "Suivez attentivement les consignes de chaque exercice. <br>\n",
    "Il s'agit d'un TP guidé. La majorité du code est dèjà fournie. et vous allez compléter quelques lignes. Lisez attentivement les explications, questions et commentaires du code pour comprendre ce qui est demandé à chaque fois"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You have here all the imports you are going \n",
    "# to need for this exercice. Please don't import \n",
    "# anything else\n",
    "from sklearn.datasets import load_boston\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercice\n",
    "Dans cet exercice nous allons travailler averc Boston Housing Data. Une dataset très connue surtout lorqu'il s'agit de régression linéaire sur Python vu qu'elle est fournie avec le package sklearn. Elle contient des données concernant des maisons dans différents quartiers de Boston. La variable qu'on cherche à prédire est la médiane des prix des maisons habitées dans un quartier. Vous aurez plus de détails dans la prochaine étape.<br><br>\n",
    "\n",
    "L'objectif est de créer 3 modèles de régression linéaire : un modèle avec LinearRegression de sklearn (modèle OLS), un modèle avec SGDRegressor (utilisant Gradient Descent) et un modèle basé aussi sur gradient descent mais que nous allons implémenter manuellement ensemble. <br><br>\n",
    "\n",
    "Dans ce TP nous allons aussi utiliser la notion de pipelines qui ont pour objectif de lier les différentes phases de la création du modèle afin de l'exécuter d'une manière séquentielle et automatiques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Etape 1: Importer les données et préparer la dataset. \n",
    "Pour cette étape, il n'y a pas de travail demandé. Tout est déjà implémenté. Par contre, je vous prie de lire cette partie parce qu'elle va servir à réaliser le travail plus tard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['data', 'target', 'feature_names', 'DESCR', 'filename'])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading the data: \n",
    "boston = load_boston()\n",
    "# Printing data keys\n",
    "boston.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'import de Boston donne un dictionnaire de 5 éléments qui sont: \n",
    "1. __data:__ les données que nous allons travailler avec sans la variable à prédire (autrement dit X)\n",
    "2. __target:__ la variable à prédire (autrement dit Y)\n",
    "3. __feature_names:__ les noms des différentes variables\n",
    "4. __'DESCR':__ la description de chaque variable\n",
    "5. __filename__: l'emplacement de la dataset boston en csv\n",
    "\n",
    "Maintenant nous allons créer X et y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the shape of X is  (506, 13)\n",
      "the shape of y is  (506,)\n"
     ]
    }
   ],
   "source": [
    "X= boston.data\n",
    "y= boston.target\n",
    "# printing the shapes of X and y\n",
    "print('the shape of X is ', X.shape)\n",
    "print('the shape of y is ', y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Maintenant, explorons les variables qui constituent notre dataset.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".. _boston_dataset:\n",
      "\n",
      "Boston house prices dataset\n",
      "---------------------------\n",
      "\n",
      "**Data Set Characteristics:**  \n",
      "\n",
      "    :Number of Instances: 506 \n",
      "\n",
      "    :Number of Attributes: 13 numeric/categorical predictive. Median Value (attribute 14) is usually the target.\n",
      "\n",
      "    :Attribute Information (in order):\n",
      "        - CRIM     per capita crime rate by town\n",
      "        - ZN       proportion of residential land zoned for lots over 25,000 sq.ft.\n",
      "        - INDUS    proportion of non-retail business acres per town\n",
      "        - CHAS     Charles River dummy variable (= 1 if tract bounds river; 0 otherwise)\n",
      "        - NOX      nitric oxides concentration (parts per 10 million)\n",
      "        - RM       average number of rooms per dwelling\n",
      "        - AGE      proportion of owner-occupied units built prior to 1940\n",
      "        - DIS      weighted distances to five Boston employment centres\n",
      "        - RAD      index of accessibility to radial highways\n",
      "        - TAX      full-value property-tax rate per $10,000\n",
      "        - PTRATIO  pupil-teacher ratio by town\n",
      "        - B        1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town\n",
      "        - LSTAT    % lower status of the population\n",
      "        - MEDV     Median value of owner-occupied homes in $1000's\n",
      "\n",
      "    :Missing Attribute Values: None\n",
      "\n",
      "    :Creator: Harrison, D. and Rubinfeld, D.L.\n",
      "\n",
      "This is a copy of UCI ML housing dataset.\n",
      "https://archive.ics.uci.edu/ml/machine-learning-databases/housing/\n",
      "\n",
      "\n",
      "This dataset was taken from the StatLib library which is maintained at Carnegie Mellon University.\n",
      "\n",
      "The Boston house-price data of Harrison, D. and Rubinfeld, D.L. 'Hedonic\n",
      "prices and the demand for clean air', J. Environ. Economics & Management,\n",
      "vol.5, 81-102, 1978.   Used in Belsley, Kuh & Welsch, 'Regression diagnostics\n",
      "...', Wiley, 1980.   N.B. Various transformations are used in the table on\n",
      "pages 244-261 of the latter.\n",
      "\n",
      "The Boston house-price data has been used in many machine learning papers that address regression\n",
      "problems.   \n",
      "     \n",
      ".. topic:: References\n",
      "\n",
      "   - Belsley, Kuh & Welsch, 'Regression diagnostics: Identifying Influential Data and Sources of Collinearity', Wiley, 1980. 244-261.\n",
      "   - Quinlan,R. (1993). Combining Instance-Based and Model-Based Learning. In Proceedings on the Tenth International Conference of Machine Learning, 236-243, University of Massachusetts, Amherst. Morgan Kaufmann.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(boston.DESCR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La dataset a une seule variable catégorique qui est déjà codée en dummy variable. Donc, on n'a pas à se soucier par rapport à ça. <br><br>\n",
    "Aussi si les variables vous semblent un peu difficiles à comprendre ne vous inquiétez pas. Notre objectif dans ce TP n'est pas d'analyser les variables mais plutôt de les utiliser pour la modélisation<br><br>\n",
    "Par contre, je vais mettre les données dans une dataframe pour les mieux visualiser et explorer quelques caractéristiques en utilisant Pandas. Notez bien, que cette manipulation est juste pour l'exploration des données et on ne va pas utiliser le données sous formes de dataframe dans le  reste de l'exercice (Rappelez-vous, on a déjà créé X et y)<br><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CRIM</th>\n",
       "      <th>ZN</th>\n",
       "      <th>INDUS</th>\n",
       "      <th>CHAS</th>\n",
       "      <th>NOX</th>\n",
       "      <th>RM</th>\n",
       "      <th>AGE</th>\n",
       "      <th>DIS</th>\n",
       "      <th>RAD</th>\n",
       "      <th>TAX</th>\n",
       "      <th>PTRATIO</th>\n",
       "      <th>B</th>\n",
       "      <th>LSTAT</th>\n",
       "      <th>PRICE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00632</td>\n",
       "      <td>18.0</td>\n",
       "      <td>2.31</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>6.575</td>\n",
       "      <td>65.2</td>\n",
       "      <td>4.0900</td>\n",
       "      <td>1.0</td>\n",
       "      <td>296.0</td>\n",
       "      <td>15.3</td>\n",
       "      <td>396.90</td>\n",
       "      <td>4.98</td>\n",
       "      <td>24.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.02731</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>6.421</td>\n",
       "      <td>78.9</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2.0</td>\n",
       "      <td>242.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>396.90</td>\n",
       "      <td>9.14</td>\n",
       "      <td>21.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.02729</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>7.185</td>\n",
       "      <td>61.1</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2.0</td>\n",
       "      <td>242.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>392.83</td>\n",
       "      <td>4.03</td>\n",
       "      <td>34.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.03237</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>6.998</td>\n",
       "      <td>45.8</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3.0</td>\n",
       "      <td>222.0</td>\n",
       "      <td>18.7</td>\n",
       "      <td>394.63</td>\n",
       "      <td>2.94</td>\n",
       "      <td>33.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.06905</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>7.147</td>\n",
       "      <td>54.2</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3.0</td>\n",
       "      <td>222.0</td>\n",
       "      <td>18.7</td>\n",
       "      <td>396.90</td>\n",
       "      <td>5.33</td>\n",
       "      <td>36.2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      CRIM    ZN  INDUS  CHAS    NOX     RM   AGE     DIS  RAD    TAX  \\\n",
       "0  0.00632  18.0   2.31   0.0  0.538  6.575  65.2  4.0900  1.0  296.0   \n",
       "1  0.02731   0.0   7.07   0.0  0.469  6.421  78.9  4.9671  2.0  242.0   \n",
       "2  0.02729   0.0   7.07   0.0  0.469  7.185  61.1  4.9671  2.0  242.0   \n",
       "3  0.03237   0.0   2.18   0.0  0.458  6.998  45.8  6.0622  3.0  222.0   \n",
       "4  0.06905   0.0   2.18   0.0  0.458  7.147  54.2  6.0622  3.0  222.0   \n",
       "\n",
       "   PTRATIO       B  LSTAT  PRICE  \n",
       "0     15.3  396.90   4.98   24.0  \n",
       "1     17.8  396.90   9.14   21.6  \n",
       "2     17.8  392.83   4.03   34.7  \n",
       "3     18.7  394.63   2.94   33.4  \n",
       "4     18.7  396.90   5.33   36.2  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating a pandas DF\n",
    "bostonDF = pd.DataFrame( boston.data )\n",
    "# adding the feature_names as column headers\n",
    "bostonDF.columns = boston.feature_names\n",
    "#adding the target variable to the dataframe\n",
    "bostonDF['PRICE' ]= boston.target\n",
    "bostonDF.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 506 entries, 0 to 505\n",
      "Data columns (total 14 columns):\n",
      " #   Column   Non-Null Count  Dtype  \n",
      "---  ------   --------------  -----  \n",
      " 0   CRIM     506 non-null    float64\n",
      " 1   ZN       506 non-null    float64\n",
      " 2   INDUS    506 non-null    float64\n",
      " 3   CHAS     506 non-null    float64\n",
      " 4   NOX      506 non-null    float64\n",
      " 5   RM       506 non-null    float64\n",
      " 6   AGE      506 non-null    float64\n",
      " 7   DIS      506 non-null    float64\n",
      " 8   RAD      506 non-null    float64\n",
      " 9   TAX      506 non-null    float64\n",
      " 10  PTRATIO  506 non-null    float64\n",
      " 11  B        506 non-null    float64\n",
      " 12  LSTAT    506 non-null    float64\n",
      " 13  PRICE    506 non-null    float64\n",
      "dtypes: float64(14)\n",
      "memory usage: 55.4 KB\n"
     ]
    }
   ],
   "source": [
    "# checking for missing values\n",
    "bostonDF.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__On n'a pas de données manquantes (yay)__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CRIM</th>\n",
       "      <th>ZN</th>\n",
       "      <th>INDUS</th>\n",
       "      <th>CHAS</th>\n",
       "      <th>NOX</th>\n",
       "      <th>RM</th>\n",
       "      <th>AGE</th>\n",
       "      <th>DIS</th>\n",
       "      <th>RAD</th>\n",
       "      <th>TAX</th>\n",
       "      <th>PTRATIO</th>\n",
       "      <th>B</th>\n",
       "      <th>LSTAT</th>\n",
       "      <th>PRICE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>3.613524</td>\n",
       "      <td>11.363636</td>\n",
       "      <td>11.136779</td>\n",
       "      <td>0.069170</td>\n",
       "      <td>0.554695</td>\n",
       "      <td>6.284634</td>\n",
       "      <td>68.574901</td>\n",
       "      <td>3.795043</td>\n",
       "      <td>9.549407</td>\n",
       "      <td>408.237154</td>\n",
       "      <td>18.455534</td>\n",
       "      <td>356.674032</td>\n",
       "      <td>12.653063</td>\n",
       "      <td>22.532806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>8.601545</td>\n",
       "      <td>23.322453</td>\n",
       "      <td>6.860353</td>\n",
       "      <td>0.253994</td>\n",
       "      <td>0.115878</td>\n",
       "      <td>0.702617</td>\n",
       "      <td>28.148861</td>\n",
       "      <td>2.105710</td>\n",
       "      <td>8.707259</td>\n",
       "      <td>168.537116</td>\n",
       "      <td>2.164946</td>\n",
       "      <td>91.294864</td>\n",
       "      <td>7.141062</td>\n",
       "      <td>9.197104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.006320</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.460000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.385000</td>\n",
       "      <td>3.561000</td>\n",
       "      <td>2.900000</td>\n",
       "      <td>1.129600</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>187.000000</td>\n",
       "      <td>12.600000</td>\n",
       "      <td>0.320000</td>\n",
       "      <td>1.730000</td>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.082045</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.190000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.449000</td>\n",
       "      <td>5.885500</td>\n",
       "      <td>45.025000</td>\n",
       "      <td>2.100175</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>279.000000</td>\n",
       "      <td>17.400000</td>\n",
       "      <td>375.377500</td>\n",
       "      <td>6.950000</td>\n",
       "      <td>17.025000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.256510</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>9.690000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.538000</td>\n",
       "      <td>6.208500</td>\n",
       "      <td>77.500000</td>\n",
       "      <td>3.207450</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>330.000000</td>\n",
       "      <td>19.050000</td>\n",
       "      <td>391.440000</td>\n",
       "      <td>11.360000</td>\n",
       "      <td>21.200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>3.677083</td>\n",
       "      <td>12.500000</td>\n",
       "      <td>18.100000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.624000</td>\n",
       "      <td>6.623500</td>\n",
       "      <td>94.075000</td>\n",
       "      <td>5.188425</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>666.000000</td>\n",
       "      <td>20.200000</td>\n",
       "      <td>396.225000</td>\n",
       "      <td>16.955000</td>\n",
       "      <td>25.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>88.976200</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>27.740000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.871000</td>\n",
       "      <td>8.780000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>12.126500</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>711.000000</td>\n",
       "      <td>22.000000</td>\n",
       "      <td>396.900000</td>\n",
       "      <td>37.970000</td>\n",
       "      <td>50.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             CRIM          ZN       INDUS        CHAS         NOX          RM  \\\n",
       "count  506.000000  506.000000  506.000000  506.000000  506.000000  506.000000   \n",
       "mean     3.613524   11.363636   11.136779    0.069170    0.554695    6.284634   \n",
       "std      8.601545   23.322453    6.860353    0.253994    0.115878    0.702617   \n",
       "min      0.006320    0.000000    0.460000    0.000000    0.385000    3.561000   \n",
       "25%      0.082045    0.000000    5.190000    0.000000    0.449000    5.885500   \n",
       "50%      0.256510    0.000000    9.690000    0.000000    0.538000    6.208500   \n",
       "75%      3.677083   12.500000   18.100000    0.000000    0.624000    6.623500   \n",
       "max     88.976200  100.000000   27.740000    1.000000    0.871000    8.780000   \n",
       "\n",
       "              AGE         DIS         RAD         TAX     PTRATIO           B  \\\n",
       "count  506.000000  506.000000  506.000000  506.000000  506.000000  506.000000   \n",
       "mean    68.574901    3.795043    9.549407  408.237154   18.455534  356.674032   \n",
       "std     28.148861    2.105710    8.707259  168.537116    2.164946   91.294864   \n",
       "min      2.900000    1.129600    1.000000  187.000000   12.600000    0.320000   \n",
       "25%     45.025000    2.100175    4.000000  279.000000   17.400000  375.377500   \n",
       "50%     77.500000    3.207450    5.000000  330.000000   19.050000  391.440000   \n",
       "75%     94.075000    5.188425   24.000000  666.000000   20.200000  396.225000   \n",
       "max    100.000000   12.126500   24.000000  711.000000   22.000000  396.900000   \n",
       "\n",
       "            LSTAT       PRICE  \n",
       "count  506.000000  506.000000  \n",
       "mean    12.653063   22.532806  \n",
       "std      7.141062    9.197104  \n",
       "min      1.730000    5.000000  \n",
       "25%      6.950000   17.025000  \n",
       "50%     11.360000   21.200000  \n",
       "75%     16.955000   25.000000  \n",
       "max     37.970000   50.000000  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# getting the dataset description\n",
    "bostonDF.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__On remarque que les variables sont de différentes échelles. Nous allons les standardiser plus tard pour assurer une meilleure modélisation__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Etape 2: Division des données en training et testing\n",
    "<font color='red'>Question 1: (1 point) </font> <br>\n",
    "Divisez les données en training et testing en utilisant la méthode 'train_test_split', en gardant 80% des données pour le training et 20% pour le test. Il faudra aussi activer le shuffle pour assurer une variété des données et choisir un random_state égal à 42 pour la reproduction des mêmes résultats pour toute exécution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "#########################################\n",
    "# write the code to split the data here #\n",
    "#########################################\n",
    "X_train, X_test, y_train, y_test= train_test_split(X, y,train_size=0.80, test_size=0.20,shuffle=True , random_state=42) # question 1\n",
    "#########################################\n",
    "#########################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Etape 3: Préparation de la dataframe regroupant les résultats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Results</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>theta0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>theta1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>theta2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>theta3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>theta4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>theta5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>theta6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>theta7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>theta8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>theta9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>theta10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>theta11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>theta12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>theta13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>train_RMSE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>test_RMSE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>r_squared</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Results\n",
       "0       theta0\n",
       "1       theta1\n",
       "2       theta2\n",
       "3       theta3\n",
       "4       theta4\n",
       "5       theta5\n",
       "6       theta6\n",
       "7       theta7\n",
       "8       theta8\n",
       "9       theta9\n",
       "10     theta10\n",
       "11     theta11\n",
       "12     theta12\n",
       "13     theta13\n",
       "14  train_RMSE\n",
       "15   test_RMSE\n",
       "16   r_squared"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# preparing the template of the dataframe to which we will append the results of each model\n",
    "results = pd.DataFrame({'Results':['theta0','theta1','theta2','theta3','theta4','theta5','theta6','theta7','theta8','theta9'\n",
    "                                            ,'theta10','theta11', 'theta12','theta13', 'train_RMSE', 'test_RMSE','r_squared']})\n",
    "\n",
    "results.head(17)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Etape 4: Création du premier modèle (LinearRegression)\n",
    "\n",
    "Les 3 premières lignes de la cellule ci-dessous montrent l'utilisation de pipeline. L'idée est de regrouper différents traitements qui se succèdent de manière à s'exécuter l'un après l'autres automatiquement sans avoir à intervenir et exécuter à chaque fois. <br>\n",
    "Par exemple ici, nous avons regroupé la préparation des données (la standardization) avec la modélisation. De cette manière, on n'a pas à exécuter la méthode fit pour la standardization puis une autre fois pour la modélisation. Il suffit de faire un seul fit qui va standardiser les données puis les fournir au modèle comme dans la lign 4 de la cellule. <br>\n",
    "Pour accéder aux méthodes et paramètres de chaque partie de la pipeline il suffit d'accéder à l'élément souhaité. \n",
    "<br>\n",
    "<font color='green'>Par exemple: sklearn_Linear_Regression_model['ols'].intercept_ </font> <br><br> <br>\n",
    "\n",
    "<font color='red'>Question 2: (1 point)  </font> <br>\n",
    "Complétez les méthodes predict afin de prédire les valeurs des données de training dans la première ligne et les valeurs des données de test dans la deuxième ligne <br> <br>\n",
    "\n",
    "<font color='red'>Question 3: (1 point)  </font> <br>\n",
    "Concaténez la dataframe 'first_model' à la dataframe 'results'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preparation and initialization of the pipeline\n",
    "sklearn_Linear_Regression_model  = Pipeline([\n",
    "        (\"std_scaler\", StandardScaler()),  # First step preprocessing\n",
    "        (\"ols\", LinearRegression())        # Second step modeling\n",
    "        ])\n",
    "\n",
    "#fitting the data and creating the model\n",
    "sklearn_Linear_Regression_model.fit(X_train, y_train)\n",
    "\n",
    "# predicting price values using the model defined above for training data and testing data\n",
    "\n",
    "#########################################\n",
    "#             Question 2                #\n",
    "#########################################\n",
    "y_pred_train = sklearn_Linear_Regression_model.predict(X_train)\n",
    "y_pred_test = sklearn_Linear_Regression_model.predict(X_test)\n",
    "#########################################\n",
    "#########################################\n",
    "\n",
    "# Calculating result metrics of the model for evaluation\n",
    "rmse_ols_train = np.sqrt(mean_squared_error(y_train, y_pred_train)) # RMSE for training data\n",
    "rmse_ols_test= np.sqrt(mean_squared_error(y_test, y_pred_test))     # RMSE for testing data\n",
    "r2_ols= r2_score(y_pred_test, y_test)                               # R squared\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Results</th>\n",
       "      <th>sklearn LinearRegression</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>theta0</td>\n",
       "      <td>22.796535</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>theta1</td>\n",
       "      <td>-1.002135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>theta2</td>\n",
       "      <td>0.696269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>theta3</td>\n",
       "      <td>0.278065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>theta4</td>\n",
       "      <td>0.718738</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>theta5</td>\n",
       "      <td>-2.022319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>theta6</td>\n",
       "      <td>3.145240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>theta7</td>\n",
       "      <td>-0.176048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>theta8</td>\n",
       "      <td>-3.081908</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>theta9</td>\n",
       "      <td>2.251407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>theta10</td>\n",
       "      <td>-1.767014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>theta11</td>\n",
       "      <td>-2.037752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>theta12</td>\n",
       "      <td>1.129568</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>theta13</td>\n",
       "      <td>-3.611658</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>train_RMSE</td>\n",
       "      <td>4.652033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>test_RMSE</td>\n",
       "      <td>4.928602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>r_squared</td>\n",
       "      <td>0.633325</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Results  sklearn LinearRegression\n",
       "0       theta0                 22.796535\n",
       "1       theta1                 -1.002135\n",
       "2       theta2                  0.696269\n",
       "3       theta3                  0.278065\n",
       "4       theta4                  0.718738\n",
       "5       theta5                 -2.022319\n",
       "6       theta6                  3.145240\n",
       "7       theta7                 -0.176048\n",
       "8       theta8                 -3.081908\n",
       "9       theta9                  2.251407\n",
       "10     theta10                 -1.767014\n",
       "11     theta11                 -2.037752\n",
       "12     theta12                  1.129568\n",
       "13     theta13                 -3.611658\n",
       "14  train_RMSE                  4.652033\n",
       "15   test_RMSE                  4.928602\n",
       "16   r_squared                  0.633325"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating an numpy array that will contain all the model\n",
    "# results: Intercept, coefficients, RMSE for train and test\n",
    "# and Rsquared. This numpy array will then be converted into \n",
    "# a dataframe and appended to the results table prepared above. \n",
    "\n",
    "# appending intercept and coefficients\n",
    "first_model_results= np.append (sklearn_Linear_Regression_model['ols'].intercept_,sklearn_Linear_Regression_model['ols'].coef_)\n",
    "\n",
    "# appending training RMSE\n",
    "first_model_results= np.append (first_model_results,rmse_ols_train)\n",
    "\n",
    "# appending testing RMSE\n",
    "first_model_results= np.append(first_model_results,rmse_ols_test)\n",
    "\n",
    "# appending R squared\n",
    "first_model_results= np.append(first_model_results,r2_ols)\n",
    "\n",
    "\n",
    "# converting the numpy array to dataframe\n",
    "first_model = pd.DataFrame({'sklearn LinearRegression':first_model_results})\n",
    "\n",
    "#########################################\n",
    "# write the code to concatenate the DFs #\n",
    "#########################################\n",
    "results = results.join(first_model) # question 3\n",
    "#########################################\n",
    "#########################################\n",
    "results.head(17)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Etape 5: Création du deuxième modèle (SGDRegressor)\n",
    "\n",
    "Maintenant, nous allons créer un deuxième modèle qui reproduit les mêmes étapes du modèle précédant mais en utilisant SGDRegressor (Stochastic Gradient Descent: the gradient of the loss is estimated each sample at a time and the model is updated along the way with a decreasing strength schedule (aka learning rate)) qui est un modèle de régression linaire basé sur Gradient Descent au lieu d'OLS\n",
    "\n",
    "\n",
    "<font color='red'>Question 4: (1 point)  </font> <br>\n",
    "Dans la pipeline, en initialisant SGDRegressor j'ai laissé eta0 (représente le learning rate) et max_iters (représente le nombre d'itérations) vides. C'est à vous de les remplir. Explorez différentes valeurs afin de produire des résultats de métriques (RMSE et R_squared) meilleurs ou proches des résultats du premier modèle. Conseil: Initialement vous pouvez mettre n'importe quelles valeurs pour vous assurer que votre code s'exécute, puis essayez d'optimiser. __Je vous prie de ne pas changer les autres paramètres__ <br> <br>\n",
    "\n",
    "<font color='red'>Question 5: (4 points)  </font> <br>\n",
    "Reproduisez les différentes étapes réalisées pour le modèle précédent afin d'obtenir une autre colonne dans la dataframe results contenant les résultats de ce modèle. __Je vous prie d'utiliser des noms de variables convenables au modèle pour assurer une bonne lisibilité et compréhention de votre code.__\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Results</th>\n",
       "      <th>sklearn LinearRegression</th>\n",
       "      <th>sklearn SGDRegression</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>theta0</td>\n",
       "      <td>22.796535</td>\n",
       "      <td>22.781870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>theta1</td>\n",
       "      <td>-1.002135</td>\n",
       "      <td>-0.984713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>theta2</td>\n",
       "      <td>0.696269</td>\n",
       "      <td>0.649556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>theta3</td>\n",
       "      <td>0.278065</td>\n",
       "      <td>0.199681</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>theta4</td>\n",
       "      <td>0.718738</td>\n",
       "      <td>0.790325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>theta5</td>\n",
       "      <td>-2.022319</td>\n",
       "      <td>-1.986355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>theta6</td>\n",
       "      <td>3.145240</td>\n",
       "      <td>3.120201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>theta7</td>\n",
       "      <td>-0.176048</td>\n",
       "      <td>-0.203708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>theta8</td>\n",
       "      <td>-3.081908</td>\n",
       "      <td>-3.091087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>theta9</td>\n",
       "      <td>2.251407</td>\n",
       "      <td>1.969674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>theta10</td>\n",
       "      <td>-1.767014</td>\n",
       "      <td>-1.309017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>theta11</td>\n",
       "      <td>-2.037752</td>\n",
       "      <td>-1.995340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>theta12</td>\n",
       "      <td>1.129568</td>\n",
       "      <td>1.115782</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>theta13</td>\n",
       "      <td>-3.611658</td>\n",
       "      <td>-3.614279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>train_RMSE</td>\n",
       "      <td>4.652033</td>\n",
       "      <td>4.658829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>test_RMSE</td>\n",
       "      <td>4.928602</td>\n",
       "      <td>4.927977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>r_squared</td>\n",
       "      <td>0.633325</td>\n",
       "      <td>0.617039</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Results  sklearn LinearRegression  sklearn SGDRegression\n",
       "0       theta0                 22.796535              22.781870\n",
       "1       theta1                 -1.002135              -0.984713\n",
       "2       theta2                  0.696269               0.649556\n",
       "3       theta3                  0.278065               0.199681\n",
       "4       theta4                  0.718738               0.790325\n",
       "5       theta5                 -2.022319              -1.986355\n",
       "6       theta6                  3.145240               3.120201\n",
       "7       theta7                 -0.176048              -0.203708\n",
       "8       theta8                 -3.081908              -3.091087\n",
       "9       theta9                  2.251407               1.969674\n",
       "10     theta10                 -1.767014              -1.309017\n",
       "11     theta11                 -2.037752              -1.995340\n",
       "12     theta12                  1.129568               1.115782\n",
       "13     theta13                 -3.611658              -3.614279\n",
       "14  train_RMSE                  4.652033               4.658829\n",
       "15   test_RMSE                  4.928602               4.927977\n",
       "16   r_squared                  0.633325               0.617039"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# preparation and initialization of the pipeline\n",
    "\n",
    "sklearn_SGD_Regression_model  = Pipeline([\n",
    "        (\"std_scaler\", StandardScaler()),  # First step preprocessing\n",
    "        ##########\n",
    "        (\"SGD_regression\", SGDRegressor(eta0 =0.0013 ,max_iter = 200, alpha=0,learning_rate='constant',shuffle=False,\n",
    "                                       penalty=None))        # Question 4\n",
    "        ##########\n",
    "        ])\n",
    "\n",
    "\n",
    "#########################################\n",
    "#             Question 5                #\n",
    "#########################################\n",
    "#fitting the data and creating the model\n",
    "sklearn_SGD_Regression_model.fit(X_train, y_train)\n",
    "\n",
    "# predicting price values using the model defined above for training data and testing data\n",
    "y_pred_train = sklearn_SGD_Regression_model.predict(X_train)\n",
    "y_pred_test = sklearn_SGD_Regression_model.predict(X_test)\n",
    "\n",
    "\n",
    "\n",
    "# Calculating result metrics of the model for evaluation\n",
    "\n",
    "rmse_sgd_train = np.sqrt(mean_squared_error(y_train, y_pred_train)) # RMSE for training data\n",
    "rmse_sgd_test= np.sqrt(mean_squared_error(y_test, y_pred_test))     # RMSE for testing data\n",
    "r2_sgd= r2_score(y_pred_test, y_test)                               # R squared\n",
    "\n",
    "# Creating an numpy array that will contain all the model\n",
    "# results: Intercept, coefficients, RMSE for train and test\n",
    "# and Rsquared. This numpy array will then be converted into \n",
    "# a dataframe and appended to the results table prepared above. \n",
    "\n",
    "\n",
    "# appending intercept and coefficients\n",
    "second_model_results= np.append (sklearn_SGD_Regression_model['SGD_regression'].intercept_,sklearn_SGD_Regression_model['SGD_regression'].coef_)\n",
    "\n",
    "# appending training RMSE\n",
    "second_model_results= np.append (second_model_results,rmse_sgd_train)\n",
    "\n",
    "# appending testing RMSE\n",
    "second_model_results= np.append(second_model_results,rmse_sgd_test)\n",
    "\n",
    "# appending R squared\n",
    "second_model_results= np.append(second_model_results,r2_sgd)\n",
    "\n",
    "# converting the numpy array to dataframe\n",
    "second_model = pd.DataFrame({'sklearn SGDRegression':second_model_results})\n",
    "\n",
    "#concatenate the DFs #\n",
    "results = results.join(second_model) # question 3\n",
    "results.head(17)\n",
    "#########################################\n",
    "#########################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpretation :\n",
    "- On remarque que la valeur de learning rate 0.0013 donne un modéle dont la valeur de R_squared est proche de celui du modéle 1 et qui performe mieux que le modéle 1 sur le test_set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Etape 6: Implémentation de la régression linéaire utilisant gradient descent manuellement\n",
    "\n",
    "Non, ne vous inquiétez pas, vous n'allez pas le faire vous même. Vous allez juste m'aider à compléter quelques trucs. <br>\n",
    "Dans la cellule ci-dessous vous allez trouver une classe 'Manual_Linear_Regression'. <br>\n",
    "Les attributs de cette classe sont: <br>\n",
    "- **coef_**         vecteur des coefficients finaux\n",
    "- **intercept_**   intercept final\n",
    "- **thetas**        vecteur de coefficients qui est mmis à jour à chaque itération\n",
    "- **cost_history**  Liste de la valeur du cout à chaque itération\n",
    "- **learning_rate** \n",
    "- **num_iters**    Le nombre d'itérations à accomplir\n",
    "\n",
    "Les méthodes de cette classe sont: <br>\n",
    "- **gradient**         Méthode pour calculer le gradient à chaque itération\n",
    "- **cost_function**    Méthode pour cacluler le cout à chaque itération suivant les formules vues en cours\n",
    "- **gradient_descent** Méthode du calcul de gradient descent \n",
    "- **fit**              Méthode pour créer le modèle à laide des données\n",
    "- **predict**          Méthode pour la prédiction des données X en utilisant le modèle créé\n",
    "\n",
    "\n",
    "<br>\n",
    "<font color='red'>Question 6: (1 point)  </font> <br>\n",
    "Complétez la formule du gradient dans la méthode 'gradient(X,y)' de la classe. Voici la formule du gradient<br> <br>\n",
    "$$gradient=\\frac{1}{m}\\sum_{i=0}^m [(h(y^i_{pred})-y^i)x^i]$$\n",
    "<font color='red'>Question 7: (1 point)  </font> <br>\n",
    "Complétez la méthode gradient descent pour le calcul du pas du gradient (gradient step) autrement dit, la mise à jour des coefficients . Voici la formule:\n",
    "$$\\theta_j=\\theta_j-\\alpha* gradient$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Manual_Linear_Regression():\n",
    "    \n",
    "    def __init__(self,learning_rate,num_iters):\n",
    "        self.coef_ = None       # final coefficients vector\n",
    "        self.intercept_ = None  # final intercept\n",
    "        self.thetas = None       # Weights/ coefficients that will be changed at each iteration\n",
    "        self.cost_history=[]    # List containing cost values at each iteration \n",
    "        self.learning_rate=learning_rate # Linear regression learning rate/ step\n",
    "        self.num_iters= num_iters    # The number of iteration of gradient descent\n",
    "    \n",
    "    \n",
    "    def gradient(self, X, y):\n",
    "        \"\"\"\n",
    "        calculating the gradient\n",
    "\n",
    "        Args:\n",
    "            self\n",
    "            X(ndarray):      train objects\n",
    "            y(ndarray):      target values for train objects\n",
    "            \n",
    "        Return:\n",
    "            gradient\n",
    "        \"\"\"\n",
    "        # Calculating y_prediction values\n",
    "        prediction = np.dot(X, self.thetas)\n",
    "        \n",
    "        # Calculating the error (the difference between the prediction and the real values)\n",
    "        error = prediction - y\n",
    "        #Calculating the gradient\n",
    "        #########################################\n",
    "        #             Question 6                #\n",
    "        #########################################\n",
    "        gradient = 2 * (X.T.dot(error)) / X.shape[0]\n",
    "        #########################################\n",
    "        #########################################\n",
    "        return gradient\n",
    "    \n",
    "    def cost_function(self,X, y):\n",
    "        \n",
    "        \"\"\"\n",
    "        calculating cost function\n",
    "\n",
    "        Args:\n",
    "            self\n",
    "            X(ndarray):      train objects\n",
    "            y(ndarray):      target values for train objects\n",
    "            \n",
    "        Return:\n",
    "            cost\n",
    "        \"\"\"\n",
    "        # Calculating the cost as the mean squared_errors \n",
    "        # divided by 2 (check the formulas in the lesson in case of confusion)\n",
    "        cost= 0.5* mean_squared_error(y_train, np.dot(X,self.thetas))\n",
    "        return cost\n",
    "    \n",
    "    \n",
    "    def gradient_descent(self,X, y):\n",
    "        \"\"\"\n",
    "        preforming the gradient descent\n",
    "\n",
    "        Args:\n",
    "            self\n",
    "            X(ndarray):      train objects\n",
    "            y(ndarray):      target values for train objects\n",
    "            \n",
    "        Return:\n",
    "            self\n",
    "        \"\"\"\n",
    "        for i in range(self.num_iters): #iterating through the number of iterations specified \n",
    "            \n",
    "            #calculating the current cost value and appending it to the list of values\n",
    "            self.cost_history.append(self.cost_function(X, y))\n",
    "            \n",
    "            # Calculating the gradient\n",
    "            grad = self.gradient(X, y)\n",
    "            # do gradient step  aka updating the weights\n",
    "            #########################################\n",
    "            #             Question 7                #\n",
    "            #########################################\n",
    "            self.thetas = self.thetas - self.learning_rate * self.gradient(X,y)\n",
    "            #########################################\n",
    "            #########################################\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        fitting a linear regression model\n",
    "\n",
    "        Args:\n",
    "            X(ndarray):      train objects\n",
    "            y(ndarray):      target values for train objects\n",
    "            \n",
    "        Return:\n",
    "            self\n",
    "        \"\"\"\n",
    "        #add a column of ones to the data to capture the intercept\n",
    "        # So we can treat the the input variables and the intercept term homogeneously \n",
    "        # from a vectorization perspective\n",
    "        X = np.c_[np.ones(X.shape[0]), X]\n",
    "        \n",
    "        # initialize if the first step\n",
    "        # here I chose the initial weights to be random and the initial\n",
    "        #intercept to be zero. but it could be any other values\n",
    "        if self.thetas is None:\n",
    "            \n",
    "            self.thetas =np.random.rand(X.shape[1])\n",
    "            self.thetas[0]=0\n",
    "        \n",
    "        \n",
    "        # do full gradient descent\n",
    "        self.gradient_descent(X, y)\n",
    "        \n",
    "        \n",
    "        # getting the final intercept and coefficients\n",
    "        self.intercept_ = self.thetas[0]\n",
    "        self.coef_ = self.thetas[1:]\n",
    "        return self\n",
    "        \n",
    "    \n",
    "        \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Make a prediction\n",
    "\n",
    "        Args:\n",
    "            X(ndarray):      objects\n",
    "        Return:\n",
    "            pred(ndarray):   predictions\n",
    "        \"\"\"\n",
    "        # check whether X has appended column of ones or not\n",
    "        if X.shape[1] == len(self.thetas):\n",
    "            pred = np.dot(X, self.thetas)\n",
    "        else:\n",
    "            #if X doesn't have a column of ones for the intercept then \n",
    "            # we won't include it in the dot product. \n",
    "            # We'll add it after the calculations of the dot product\n",
    "            pred = np.dot(X, self.coef_) + self.intercept_\n",
    "        return pred\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Etape 7: Création du troisième modèle (Manual gradient descent)\n",
    "\n",
    "Maintenant, nous allons créer le troisième modèle qui reproduit les mêmes étapes du modèle précédant mais en utilisant le régresseur que nous avons implémenté ci-dessus qui est un modèle de régression linaire basé sur Gradient Descent au lieu d'OLS\n",
    "\n",
    "\n",
    "<font color='red'>Question 8: (1 point)  </font> <br>\n",
    "Complétez l'initialisation de \"manual_regressor\" dans la pipeline en ajoutant un learning rate=0.1 et un nombre d'itérations égal à 1000 <br> <br>\n",
    "\n",
    "<font color='red'>Question 9: (1 points)  </font> <br>\n",
    "Que pensez-vous des résultats des 3 modèles? Expliquez\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preparation and initialization of the pipeline\n",
    "manual_Regression_model  = Pipeline([\n",
    "        (\"std_scaler\", StandardScaler()),  # First step preprocessing\n",
    "         ##########\n",
    "        (\"manual_regressor\", Manual_Linear_Regression(0.003 , 1000))        # Question 8\n",
    "        ###########\n",
    "        ])\n",
    "\n",
    "#fitting the data and creating the model\n",
    "manual_Regression_model.fit(X_train, y_train)\n",
    "\n",
    "# predicting price values using the model defined above for training data and testing data\n",
    "\n",
    "\n",
    "y_pred_train = manual_Regression_model.predict(X_train)\n",
    "y_pred_test = manual_Regression_model.predict(X_test)\n",
    "\n",
    "\n",
    "# Calculating result metrics of the model for evaluation\n",
    "rmse_manual_train = np.sqrt(mean_squared_error(y_train, y_pred_train)) # RMSE for training data\n",
    "rmse_manual_test= np.sqrt(mean_squared_error(y_test, y_pred_test))     # RMSE for testing data\n",
    "r2_manual= r2_score(y_pred_test, y_test)                               # R squared\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Results</th>\n",
       "      <th>sklearn LinearRegression</th>\n",
       "      <th>sklearn SGDRegression</th>\n",
       "      <th>Manual linear regression</th>\n",
       "      <th>Manual linear regression</th>\n",
       "      <th>Manual linear regression</th>\n",
       "      <th>Manual linear regression</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>theta0</td>\n",
       "      <td>22.796535</td>\n",
       "      <td>22.781870</td>\n",
       "      <td>19.717532</td>\n",
       "      <td>21.109078</td>\n",
       "      <td>22.382337</td>\n",
       "      <td>22.741040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>theta1</td>\n",
       "      <td>-1.002135</td>\n",
       "      <td>-0.984713</td>\n",
       "      <td>-0.721625</td>\n",
       "      <td>-0.699945</td>\n",
       "      <td>-0.782618</td>\n",
       "      <td>-0.765308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>theta2</td>\n",
       "      <td>0.696269</td>\n",
       "      <td>0.649556</td>\n",
       "      <td>0.328122</td>\n",
       "      <td>0.122757</td>\n",
       "      <td>0.316252</td>\n",
       "      <td>0.285553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>theta3</td>\n",
       "      <td>0.278065</td>\n",
       "      <td>0.199681</td>\n",
       "      <td>-0.397689</td>\n",
       "      <td>-0.346304</td>\n",
       "      <td>-0.184290</td>\n",
       "      <td>0.023082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>theta4</td>\n",
       "      <td>0.718738</td>\n",
       "      <td>0.790325</td>\n",
       "      <td>0.996708</td>\n",
       "      <td>0.917631</td>\n",
       "      <td>0.830625</td>\n",
       "      <td>0.775390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>theta5</td>\n",
       "      <td>-2.022319</td>\n",
       "      <td>-1.986355</td>\n",
       "      <td>-0.352176</td>\n",
       "      <td>-0.360150</td>\n",
       "      <td>-0.780242</td>\n",
       "      <td>-1.232857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>theta6</td>\n",
       "      <td>3.145240</td>\n",
       "      <td>3.120201</td>\n",
       "      <td>3.455343</td>\n",
       "      <td>3.773864</td>\n",
       "      <td>3.566621</td>\n",
       "      <td>3.493835</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>theta7</td>\n",
       "      <td>-0.176048</td>\n",
       "      <td>-0.203708</td>\n",
       "      <td>0.401363</td>\n",
       "      <td>-0.107606</td>\n",
       "      <td>-0.099845</td>\n",
       "      <td>-0.060337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>theta8</td>\n",
       "      <td>-3.081908</td>\n",
       "      <td>-3.091087</td>\n",
       "      <td>-0.724713</td>\n",
       "      <td>-1.112468</td>\n",
       "      <td>-1.807481</td>\n",
       "      <td>-2.048514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>theta9</td>\n",
       "      <td>2.251407</td>\n",
       "      <td>1.969674</td>\n",
       "      <td>0.351302</td>\n",
       "      <td>0.127158</td>\n",
       "      <td>0.706513</td>\n",
       "      <td>0.918211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>theta10</td>\n",
       "      <td>-1.767014</td>\n",
       "      <td>-1.309017</td>\n",
       "      <td>-0.314900</td>\n",
       "      <td>-0.120297</td>\n",
       "      <td>-0.579144</td>\n",
       "      <td>-0.650772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>theta11</td>\n",
       "      <td>-2.037752</td>\n",
       "      <td>-1.995340</td>\n",
       "      <td>-1.533583</td>\n",
       "      <td>-1.652105</td>\n",
       "      <td>-1.694520</td>\n",
       "      <td>-1.845975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>theta12</td>\n",
       "      <td>1.129568</td>\n",
       "      <td>1.115782</td>\n",
       "      <td>1.243854</td>\n",
       "      <td>1.237983</td>\n",
       "      <td>1.135743</td>\n",
       "      <td>1.156493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>theta13</td>\n",
       "      <td>-3.611658</td>\n",
       "      <td>-3.614279</td>\n",
       "      <td>-3.026063</td>\n",
       "      <td>-2.873354</td>\n",
       "      <td>-3.362960</td>\n",
       "      <td>-3.497561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>train_RMSE</td>\n",
       "      <td>4.652033</td>\n",
       "      <td>4.658829</td>\n",
       "      <td>5.793798</td>\n",
       "      <td>5.145078</td>\n",
       "      <td>4.764100</td>\n",
       "      <td>4.709370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>test_RMSE</td>\n",
       "      <td>4.928602</td>\n",
       "      <td>4.927977</td>\n",
       "      <td>6.243323</td>\n",
       "      <td>5.671203</td>\n",
       "      <td>5.182990</td>\n",
       "      <td>5.116641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>r_squared</td>\n",
       "      <td>0.633325</td>\n",
       "      <td>0.617039</td>\n",
       "      <td>0.286266</td>\n",
       "      <td>0.436354</td>\n",
       "      <td>0.557263</td>\n",
       "      <td>0.575627</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Results  sklearn LinearRegression  sklearn SGDRegression  \\\n",
       "0       theta0                 22.796535              22.781870   \n",
       "1       theta1                 -1.002135              -0.984713   \n",
       "2       theta2                  0.696269               0.649556   \n",
       "3       theta3                  0.278065               0.199681   \n",
       "4       theta4                  0.718738               0.790325   \n",
       "5       theta5                 -2.022319              -1.986355   \n",
       "6       theta6                  3.145240               3.120201   \n",
       "7       theta7                 -0.176048              -0.203708   \n",
       "8       theta8                 -3.081908              -3.091087   \n",
       "9       theta9                  2.251407               1.969674   \n",
       "10     theta10                 -1.767014              -1.309017   \n",
       "11     theta11                 -2.037752              -1.995340   \n",
       "12     theta12                  1.129568               1.115782   \n",
       "13     theta13                 -3.611658              -3.614279   \n",
       "14  train_RMSE                  4.652033               4.658829   \n",
       "15   test_RMSE                  4.928602               4.927977   \n",
       "16   r_squared                  0.633325               0.617039   \n",
       "\n",
       "    Manual linear regression  Manual linear regression  \\\n",
       "0                  19.717532                 21.109078   \n",
       "1                  -0.721625                 -0.699945   \n",
       "2                   0.328122                  0.122757   \n",
       "3                  -0.397689                 -0.346304   \n",
       "4                   0.996708                  0.917631   \n",
       "5                  -0.352176                 -0.360150   \n",
       "6                   3.455343                  3.773864   \n",
       "7                   0.401363                 -0.107606   \n",
       "8                  -0.724713                 -1.112468   \n",
       "9                   0.351302                  0.127158   \n",
       "10                 -0.314900                 -0.120297   \n",
       "11                 -1.533583                 -1.652105   \n",
       "12                  1.243854                  1.237983   \n",
       "13                 -3.026063                 -2.873354   \n",
       "14                  5.793798                  5.145078   \n",
       "15                  6.243323                  5.671203   \n",
       "16                  0.286266                  0.436354   \n",
       "\n",
       "    Manual linear regression  Manual linear regression  \n",
       "0                  22.382337                 22.741040  \n",
       "1                  -0.782618                 -0.765308  \n",
       "2                   0.316252                  0.285553  \n",
       "3                  -0.184290                  0.023082  \n",
       "4                   0.830625                  0.775390  \n",
       "5                  -0.780242                 -1.232857  \n",
       "6                   3.566621                  3.493835  \n",
       "7                  -0.099845                 -0.060337  \n",
       "8                  -1.807481                 -2.048514  \n",
       "9                   0.706513                  0.918211  \n",
       "10                 -0.579144                 -0.650772  \n",
       "11                 -1.694520                 -1.845975  \n",
       "12                  1.135743                  1.156493  \n",
       "13                 -3.362960                 -3.497561  \n",
       "14                  4.764100                  4.709370  \n",
       "15                  5.182990                  5.116641  \n",
       "16                  0.557263                  0.575627  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating an numpy array that will contain all the model\n",
    "# results: Intercept, coefficients, RMSE for train and test\n",
    "# and Rsquared. This numpy array will then be converted into \n",
    "# a dataframe and appended to the results table prepared above. \n",
    "\n",
    "# appending intercept and coefficients\n",
    "third_model_results= np.append (manual_Regression_model['manual_regressor'].intercept_,manual_Regression_model['manual_regressor'].coef_)\n",
    "\n",
    "# appending training RMSE\n",
    "third_model_results= np.append (third_model_results,rmse_manual_train)\n",
    "\n",
    "# appending testing RMSE\n",
    "third_model_results= np.append(third_model_results,rmse_manual_test)\n",
    "\n",
    "# appending R squared\n",
    "third_model_results= np.append(third_model_results,r2_manual)\n",
    "\n",
    "\n",
    "# converting the numpy array to dataframe\n",
    "third_model = pd.DataFrame({'Manual linear regression':third_model_results})\n",
    "\n",
    "\n",
    "results = pd.concat([results,third_model], axis=1, sort=False)\n",
    "\n",
    "results.head(17)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpretation :\n",
    "- le modéle 1 donne le meilleur valeur de R_squared, le modéle 2 a une valeur de R squared trés proche du modéle 1 mais il performe mieux sur le test set, le modéle 3 a une valeur de r_squared inférieur aux autres et donne une erreur plus élevé."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Etape 8: Visualisation de la fonction de cout\n",
    "\n",
    "\n",
    "\n",
    "<font color='red'>Question 10: (1 point)  </font> <br>\n",
    "En utilisant le regresseur 'manual_regression' et en exploitant son attribut 'cost_history' visualisez la courbe de la fonction du cout à travers les différetes itérations <br> <br>\n",
    "\n",
    "<font color='red'>Question 11: (1 points)  </font> <br>\n",
    "Que pensez-vous de la fonction de cout visualisée? A-t-elle le comportement attendu? Comment? \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'Cost J')"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x504 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot the convergence graph\n",
    "plt.figure(figsize=(15,7))\n",
    "#########################################\n",
    "#             Question 10               #\n",
    "#########################################\n",
    "plt.plot(manual_Regression_model['manual_regressor'].cost_history)\n",
    "#########################################\n",
    "#########################################\n",
    "plt.xlabel('Number of iterations')\n",
    "plt.ylabel('Cost J')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=blue>Interpretation:</font> \n",
    "- La fonction cout est trés élevée dans les premiéres itérations puis décroit  jusqu'a converger vers une valeur de 309.76, ceci montre que le minimum local a été atteint et que les paramétres ne seront plus mis a jour au déla de cette valeur. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
